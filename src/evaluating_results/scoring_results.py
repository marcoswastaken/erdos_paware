import os

import polars as pl

import paware

def compute_rr_scores(results_dir: str):
    """
    Compute the reciprocal rank scores for the results in the results_dir and 
    return the scores in a DataFrame.

    Parameters:
        results_dir (str): The directory containing the results files. 
        These are expected to be .parquet files generated by the
        PawQuery.ask_standard_queries() method.
    """
    
    ## Get the list of files in the directory
    files = [f for f in os.listdir(results_dir) if f[0]!='.']

    ## Load the score tool
    score_tool = paware.PawScores(results_dir+files[0])

    ## Compute the scores
    score_tool.compute_rr_scores()

    ## Get the scores
    query_scores = score_tool.get_rr_scores()

    ## Create the DataFrame
    rr_df = pl.DataFrame({"query":query_scores.keys(), 
                          "rr_score_"+files[0]\
                            .split(".")[0]:query_scores.values()})

    ## Loop through the other files
    for f in files[1:]:
        score_tool = paware.PawScores(results_dir+f)
        score_tool.compute_rr_scores()
        query_scores = score_tool.get_rr_scores()
        current =  pl.DataFrame(
            {"query":query_scores.keys(),
             "rr_score_"+f.split(".")[0]:query_scores.values()})
        
        ## Append the scores from each file to the DataFrame
        rr_df = rr_df.join(current, on="query", how="left")

    return rr_df

def compute_mext_rr_scores(results_dir: str):
    """
    Compute the reciprocal rank scores for the results in the results_dir and 
    return the scores in a DataFrame.

    Parameters:
        results_dir (str): The directory containing the results files. 
        These are expected to be .parquet files generated by the
        PawQuery.ask_standard_queries() method.
    """
    
    ## Get the list of files in the directory
    files = [f for f in os.listdir(results_dir) if f[0]!='.']

    ## Load the score tool
    score_tool = paware.PawScores(results_dir+files[0])

    ## Compute the scores
    score_tool.compute_mext_rr_scores()

    ## Get the scores
    query_scores = score_tool.get_mext_rr_scores()

    ## Create the DataFrame
    mext_rr_df = pl.DataFrame({"query":query_scores.keys(), 
                               "mext_rr_score_"+files[0]\
                                .split(".")[0]:query_scores.values()})

    ## Loop through the other files
    for f in files[1:]:
        score_tool = paware.PawScores(results_dir+f)
        score_tool.compute_mext_rr_scores()
        query_scores = score_tool.get_mext_rr_scores()
        current =  pl.DataFrame(
            {"query":query_scores.keys(),
             "mext_rr_score_"+f.split(".")[0]:query_scores.values()})
        
        ## Append the scores from each file to the DataFrame
        mext_rr_df = mext_rr_df.join(current, on="query", how="left")

    return mext_rr_df

def compute_dcg_scores(results_dir: str):
    """
    Compute the reciprocal rank scores for the results in the results_dir and 
    return the scores in a DataFrame.

    Parameters:
        results_dir (str): The directory containing the results files. 
        These are expected to be .parquet files generated by the
        PawQuery.ask_standard_queries() method.
    """
    
    ## Get the list of files in the directory
    files = [f for f in os.listdir(results_dir) if f[0]!='.']

    ## Load the score tool
    score_tool = paware.PawScores(results_dir+files[0])

    ## Compute the scores
    score_tool.compute_dcg_scores()

    ## Get the scores
    query_scores = score_tool.get_dcg_scores()

    ## Create the DataFrame
    dcg_df = pl.DataFrame({"query":query_scores.keys(), 
                          "dcg_score_"+files[0]\
                            .split(".")[0]:query_scores.values()})

    ## Loop through the other files
    for f in files[1:]:
        score_tool = paware.PawScores(results_dir+f)
        score_tool.compute_dcg_scores()
        query_scores = score_tool.get_dcg_scores()
        current =  pl.DataFrame(
            {"query":query_scores.keys(),
             "dcg_score_"+f.split(".")[0]:query_scores.values()})
        
        ## Append the scores from each file to the DataFrame
        dcg_df = dcg_df.join(current, on="query", how="left")

    return dcg_df

def paw_config_score_summary(
        rr_scores: pl.DataFrame,
        mext_rr_scores: pl.DataFrame,
        dcg_scores: pl.DataFrame):
    
    ## Organize scores into columns
    rr_scores = rr_scores.mean().transpose(include_header=True)
    mext_rr_scores = mext_rr_scores.mean().transpose(include_header=True)
    dcg_scores = dcg_scores.mean().transpose(include_header=True)

    ## Rename columns
    rr_scores = rr_scores.rename({"column":"query_configuration", 
                                "column_0":"mean_rr_score"})
    mext_rr_scores = mext_rr_scores.rename({"column":"query_configuration", 
                                            "column_0":"mean_mext_rr_score"})
    dcg_scores = dcg_scores.rename({"column":"query_configuration", 
                                    "column_0":"mean_dcg_score"})

    ## Break down query configuration into its components
    rr_scores  = rr_scores.with_columns(
            pl.col("query_configuration").str.slice(-9).alias("config_code"),
            pl.col("query_configuration").str.slice(-9,2).alias("emb"),
            pl.col("query_configuration").str.slice(-5,1).alias("filter_short_qs"),
            pl.col("query_configuration").str.slice(-4,1).alias("filter_submissions"),
            pl.col("query_configuration").str.slice(-3,1).alias("rerank_sentiment"),
            pl.col("query_configuration").str.slice(-2,1).alias("rerank_agree"),
            pl.col("query_configuration").str.slice(-1,1).alias("rerank_disagree")
            ).filter(pl.col("query_configuration")!="query").clone()

    rr_scores = rr_scores.sort(by="mean_rr_score", descending=True)\
            .with_row_index("rr_rank", offset=1).clone()

    mext_rr_scores  = mext_rr_scores.with_columns(
            pl.col("query_configuration").str.slice(-9).alias("config_code"),
            pl.col("query_configuration").str.slice(-9,2).alias("emb"),
            pl.col("query_configuration").str.slice(-5,1).alias("filter_short_qs"),
            pl.col("query_configuration").str.slice(-4,1).alias("filter_submissions"),
            pl.col("query_configuration").str.slice(-3,1).alias("rerank_sentiment"),
            pl.col("query_configuration").str.slice(-2,1).alias("rerank_agree"),
            pl.col("query_configuration").str.slice(-1,1).alias("rerank_disagree")
            ).filter(pl.col("query_configuration")!="query").clone()

    mext_rr_scores = mext_rr_scores.sort(by="mean_mext_rr_score", descending=True)\
            .with_row_index("mext_rr_rank", offset=1).clone()

    dcg_scores  = dcg_scores.with_columns(
            pl.col("query_configuration").str.slice(-9).alias("config_code"),
            pl.col("query_configuration").str.slice(-9,2).alias("emb"),
            pl.col("query_configuration").str.slice(-5,1).alias("filter_short_qs"),
            pl.col("query_configuration").str.slice(-4,1).alias("filter_submissions"),
            pl.col("query_configuration").str.slice(-3,1).alias("rerank_sentiment"),
            pl.col("query_configuration").str.slice(-2,1).alias("rerank_agree"),
            pl.col("query_configuration").str.slice(-1,1).alias("rerank_disagree")
            ).filter(pl.col("query_configuration")!="query").clone()

    dcg_scores = dcg_scores.sort(by="mean_dcg_score", descending=True)\
            .with_row_index("dcg_rank", offset=1).clone()

    ## Reorder columns
    rr_col_order = ["config_code"]+[col for col in rr_scores.columns 
                                    if (col != "config_code") 
                                    and (col != "query_configuration")]

    mext_rr_col_order = ["config_code"]+[col for col in mext_rr_scores.columns if 
                                        (col != "config_code") 
                                        and (col != "query_configuration")]

    dcg_col_order = ["config_code"]+[col for col in dcg_scores.columns 
                                    if (col != "config_code") 
                                    and (col != "query_configuration")]

    rr_scores = rr_scores[rr_col_order].clone()
    mext_rr_scores = mext_rr_scores[mext_rr_col_order].clone()
    dcg_scores = dcg_scores[dcg_col_order].clone()

    ## Join the scores into a single dataframe
    score_summary = rr_scores.join(
        mext_rr_scores[["config_code", "mean_mext_rr_score", "mext_rr_rank"]], 
        on="config_code", how="left").join(
            dcg_scores[["config_code", "mean_dcg_score", "dcg_rank"]], 
            on="config_code", how="left")

    score_summary = score_summary.with_columns(
            pl.col("mean_rr_score").cast(pl.Float32),
            pl.col("mean_mext_rr_score").cast(pl.Float32),
            pl.col("mean_dcg_score").cast(pl.Float32),
            pl.col("filter_short_qs").cast(pl.Int32),
            pl.col("filter_submissions").cast(pl.Int32),
            pl.col("rerank_sentiment").cast(pl.Int32),
            pl.col("rerank_agree").cast(pl.Int32),
            pl.col("rerank_disagree").cast(pl.Int32))

    score_summary = score_summary.with_columns(
        mean_rank = (pl.col("rr_rank")+pl.col("mext_rr_rank")+pl.col("dcg_rank"))/3)

    score_summary = score_summary[[
            'config_code',
            'mean_mext_rr_score',
            'mean_dcg_score',
            'mean_rr_score',
            'rr_rank',
            'mext_rr_rank',
            'dcg_rank',
            'mean_rank',
            'emb',
            'filter_short_qs',
            'filter_submissions',
            'rerank_sentiment',
            'rerank_agree',
            'rerank_disagree']].clone()
    
    return score_summary
