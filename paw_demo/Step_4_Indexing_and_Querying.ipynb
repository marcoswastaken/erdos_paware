{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add this directory to the path and load our functions\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "import paware\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing the Data\n",
    "\n",
    "Now that we have vector representation of all of our data, we can load it into a vector database for streamlined access through indexing.\n",
    "\n",
    "We are using [LanceDB](https://lancedb.github.io/lancedb/) for our database needs, and we are taking advantage of its built in Almost Nearest Neighbors (ANN) index capabilities, provided as a composite inverted file index with product quantization ((IVF_PQ)[https://lancedb.github.io/lancedb/concepts/index_ivfpq/]).\n",
    "\n",
    "The purpose of indexing is to improve retrieval times, while trading off accuracy. We found a set of parameters that worked well for our purposes here, but its possible that we could achieve even faster retrieval with minimal loss in performance by adjusting them.\n",
    "\n",
    "For this demonstration, we chose the following parameters:\n",
    "\n",
    "* `EMBEDDING_CONFIG_NAME = demo`: This specifies that we are going to index the \"demo\" configuration, that we have already processed the embeddings for and to which we have attached our engineered metadata.\n",
    "* `EMBEDDING_DIR =  \"../paw_demo/embedded_subs\"`: This tells our indexer where to find the embedded data.\n",
    "* `INDEX_CONFIG_NAME = \"_demo\"`: This attaches a label for this specific embedding configuration. In testing, we can index the same data using diffferent indexing parameters, and this allows us to differentiate between them.\n",
    "* `DB_SAVE_DIR = \"../paw_demo/indexed_data\"`: This specifies where to save the database table once indexing is complete.\n",
    "* `METRIC = \"cosine\"`: Here we chose the metric to be used by the index, our default is `cosine`\n",
    "* `NUM_PARTITIONS  = 128`: Here we specify how many partitions should be created by the IVF. Our default is 1024 on the full dataset, but for this demo we'll set it much smaller.\n",
    "* `NUM_SUB_VECTORS = 24`: Here we decide the level of PQ reduction. Our default on the full dataset was 96, and again we chose a smaller number for this demo.\n",
    "* `ACCELERATOR = 'mps'`: LanceDB supports `'mps'` and `'cuda'` options here. The defaul is `None`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [00:02<00:01, 12.38it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c604069a3c6946c6b382edea99e9392e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcos/miniconda3/envs/aware/lib/python3.11/site-packages/lance/torch/data.py:67: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:212.)\n",
      "  tensor = torch.from_numpy(arr.to_numpy(zero_copy_only=False))\n"
     ]
    }
   ],
   "source": [
    "## Set up the indexing tool\n",
    "indexing_tool = paware.PawIndex(\n",
    "    EMBEDDING_CONFIG_NAME = \"demo\",\n",
    "    EMBEDDING_DIR =  \"../paw_demo/embedded_subs/\",\n",
    "    INDEX_CONFIG_NAME = \"_demo\",\n",
    "    DB_SAVE_DIR = \"../paw_demo/indexed_db\",\n",
    "    METRIC = \"cosine\",\n",
    "    NUM_PARTITIONS  = 128,\n",
    "    NUM_SUB_VECTORS = 96,\n",
    "    ACCELERATOR = 'mps',\n",
    "    )\n",
    "\n",
    "## Index the data\n",
    "indexing_tool.index_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying the Index\n",
    "\n",
    "Now the the data is indexed, we can start to query it. At this stage that we can chose to implement several of our strategies for improving the relevance of results. The following parameters define the basic behavior of query and how it will use the index.\n",
    "\n",
    "* `CONFIG_NAME`: Specifies which indexing configuration we will be querying\n",
    "* `DB_DIR`: Specifies the directory containing the database\n",
    "* `QUERY_SAVE_DIR`: Specifies where to save query results* \n",
    "* `QUERY_NAME`: Used to keep track of this query configuration for later evaluation.*\n",
    "* `LIMIT`: This specifies how many results to return,\n",
    "* `NPROBES`: This specified how many nearby partitions (created by the IVF) should be visited while looking for results,\n",
    "* `REFINE_FACTOR`: This is a multiplier that tells the index to retrieve `LIMIT*REFINE_FACTOR` results, then re-rank using the actual (non-quantized) distances before returning the top `LIMIT` as the final set.\n",
    "* `METRIC`: This is the metric used for retreival. The default we use is `cosine`.\n",
    "\n",
    "The parameters `QUERY_SAVE_DIR` and `QUERY_NAME` are only used when we ask the standard set of queries we use to score our results.\n",
    "\n",
    "## Query Parameters (Pre-query)\n",
    "\n",
    "Prior to retrieval, we can narrow down our corpus of text through pre-filtering. Under the hood, this is handled through the database using SQL queries. Our two built in filtering options are:\n",
    "\n",
    "* `FILTER_SUBMISSIONS` : This filters out any rows that have `'submission'` as their `aware_post_type`. This information was provided with the raw data.\n",
    "* `FILTER_SHORT_QUESTIONS` : This filters out any rows where the `reddit_text` is shorter than 100 characters, and end in a `?`. We added this information during preprocessing.\n",
    "\n",
    "## Query Parmeters (Post-Query)\n",
    "\n",
    "After retrieving our results, we can then re-rank them before returning them to the user. We have three re-ranking strategies we could apply, each of which depends on the engineered metadata that we've generated.\n",
    "\n",
    "* `RERANK_SENTIMENT`: This implements our re-ranking by `summed_sentiment_of_replies`\n",
    "* `RERANK_AGREE_DISTANCE`: This implements our re-ranking by `avg_reply_agree_distance`\n",
    "* `RERANK_DISAGREE_DISTANCE`: This implements our re-ranking by `avg_reply_disagree_distance`\n",
    "\n",
    "\n",
    "Below, we choose the parameters of our top performing configuration on the whole dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tool = paware.PawQuery(\n",
    "    CONFIG_NAME = \"demo_demo\",\n",
    "    DB_DIR = \"../paw_demo/indexed_db/\",\n",
    "    QUERY_SAVE_DIR = \"../paw_demo/demo_query_results/\",\n",
    "    QUERY_NAME = \"top_config\",\n",
    "    LIMIT = 20, ## Default is 50, smaller for demo\n",
    "    NPROBES =5, ## Default is 20, smaller for demo\n",
    "    REFINE_FACTOR = 5, ## Default is 10, smaller for demo\n",
    "    FILTER_SUBMISSIONS = False,\n",
    "    FILTER_SHORT_QUESTIONS = True,\n",
    "    RERANK_SENTIMENT = True,\n",
    "    RERANK_AGREE_DISTANCE= True,\n",
    "    RERANK_DISAGREE_DISTANCE = False,\n",
    "    METRIC = \"cosine\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can query the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = query_tool.ask_a_query(\"What is the best way to cook a steak?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌──────────────────┬───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
      "│ reddit_subreddit ┆ reddit_text                                                                                                                                       ┆ _distance │\n",
      "│ ---              ┆ ---                                                                                                                                               ┆ ---       │\n",
      "│ str              ┆ str                                                                                                                                               ┆ f32       │\n",
      "╞══════════════════╪═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════╪═══════════╡\n",
      "│ wholefoods       ┆ Steak tartare is great.                                                                                                                           ┆ 0.159283  │\n",
      "│ wholefoods       ┆ Talk to your butcher. I’m a Whole Foods butcher and I always keep a lil something special for my coworkers. Maybe it’s the last hanger steak, a   ┆ 0.154551  │\n",
      "│                  ┆ well marbled flat iron, a Chuck eye steak or thin cut boneless short ribs.                                                                        ┆           │\n",
      "│                  ┆ I know it’s not a steak cut, but beef shank is still only $6/lb and cooks up like…                                                                ┆           │\n",
      "│ wholefoods       ┆ I can guarantee the cut of a T-bone steak by sticking my head up the cows ass but I’d rather take the butcher’s word for it!!                     ┆ 0.166373  │\n",
      "│ wholefoods       ┆ So in the south region, don't know but the others, bone in ribeye steaks are on sale for dadays. So of course we have lots of them in the back.   ┆ 0.17207   │\n",
      "│                  ┆ And we have a pan of steaks in the case and an extra pan prepped, 3 layers of steaks each pan. 3 case worth.                                      ┆           │\n",
      "│                  ┆                                                                                                                                                   ┆           │\n",
      "│                  ┆ A guy came today just before noon and said \"g…                                                                                                    ┆           │\n",
      "│ wholefoods       ┆ As a person who cooks steak on the regular, I started getting a Good Chop box. Price is reasonable and the meats are very good. Or check out      ┆ 0.170912  │\n",
      "│                  ┆ local butchers in your area. Way better chance of being local and prices will be way under the WF markup.                                         ┆           │\n",
      "└──────────────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "with pl.Config(tbl_rows=26, tbl_width_chars=180, fmt_str_lengths=300):\n",
    "    print(results[[\"reddit_subreddit\",\"reddit_text\", \"_distance\"]].head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aware",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
