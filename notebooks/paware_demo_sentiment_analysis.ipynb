{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Notebook\n",
    "\n",
    "This sentiment analysis notebook is designed to process a collection of Reddit posts stored in Parquet files, evaluate the sentiment of each post, and aggregate the results by subreddit.\n",
    "\n",
    "The sentiment analysis is performed using a pre-trained model, `\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"`. Each batch of tokenized text is fed into the model, which outputs logits. These logits are then converted into categorical sentiment values (-1 for negative, 0 for neutral, and 1 for positive).\n",
    "\n",
    "The add_summed_sentiments function adds a column that sums up the sentiments of replies to each post, providing a more aggregated view of the sentiment towards each post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from adding_metadata.replies import add_reply_list\n",
    "from adding_metadata.reply_sentiments import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the data\n",
    "##Location of reddit.parquet\n",
    "base_dir = Path(\"../\") \n",
    "\n",
    "##To store the data splits\n",
    "data_dir = Path(base_dir,\"raw_data\") \n",
    "results_dir = Path(base_dir, \"sentiment_data\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = get_all_files(data_dir)\n",
    "all_files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sentiment analysis on each file and save the results\n",
    "for file in all_files:\n",
    "    data = TextLoader(file=file, tokenizer=tokenizer)\n",
    "    train_dataloader = DataLoader(data, batch_size=8, shuffle=False)\n",
    "    all_sentiments = []\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        input = data.to(device_staging)\n",
    "        res = model(input)\n",
    "        logits=res['logits']\n",
    "        sentiments = logits.argmax(dim=1).cpu().numpy()\n",
    "        sentiments = np.where(sentiments == 0, 1, np.where(sentiments == 1, -1, 0))\n",
    "        all_sentiments.extend(sentiments)\n",
    " \n",
    "    df = pl.read_parquet(file)\n",
    "    df = df.with_columns(pl.Series(\"text_sentiment\", all_sentiments))\n",
    "    df = add_summed_sentiments(df)\n",
    "    subreddit = file.stem.split('_')[1].lower()\n",
    "    output_path = os.path.join(results_dir, f\"sentiment_{subreddit}.parquet\")\n",
    "    df.write_parquet(output_path, compression='zstd')\n",
    "\n",
    "    del data, train_dataloader, input, res, df\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
